"""
Functions for network matched-filter detection of seismic data.

Designed to cross-correlate templates generated by template_gen function
with data and output the detections.

:copyright:
    EQcorrscan developers.

:license:
    GNU Lesser General Public License, Version 3
    (https://www.gnu.org/copyleft/lesser.html)
"""
import os
import pickle
import tempfile
import time
import traceback
import logging
import numpy as np

from typing import List, Union, Iterable
from timeit import default_timer

from multiprocessing import Queue
from queue import Empty

from obspy import Stream

from eqcorrscan.core.match_filter.helpers import (
    _pickle_stream, _unpickle_stream)
from eqcorrscan.core.match_filter.helpers.tribe import (
    _download_st, _pre_process, _group, _detect,
    _read_template_db, _make_party)

from eqcorrscan.utils.correlate import (
    _get_array_dicts, _fmf_stabilisation, _fmf_reshape)
from eqcorrscan.utils.pre_processing import (
    _quick_copy_stream, _prep_data_for_correlation)


Logger = logging.getLogger(__name__)


###############################################################################
#                           Process handlers
###############################################################################


class Poison(Exception):
    def __init__(self, value):
        """
        Poison Exception.
        """
        self.value = value

    def __repr__(self):
        return self.value

    def __str__(self):
        return self.value


def _get_and_check(input_queue: Queue, poison_queue: Queue, step: float = 0.5):
    """
    Get from a queue and check for poison - returns Poisoned if poisoned.

    :param input_queue: Queue to get something from
    :param poison_queue: Queue to check for poison

    :return: Item from queue or Poison.
    """
    while True:
        poison = _check_for_poison(poison_queue)
        if poison:
            return poison
        if input_queue.empty():
            time.sleep(step)
        else:
            return input_queue.get_nowait()


def _check_for_poison(poison_queue: Queue) -> Union[Poison, None]:
    """
    Check if poison has been added to the queue.
    """
    Logger.debug("Checking for poison")
    try:
        poison = poison_queue.get_nowait()
    except Empty:
        return
    # Put the poison back in the queue for another process to check on
    Logger.error("Poisoned")
    poison_queue.put(poison)
    return Poison(poison)


def _get_detection_stream(
    template_channel_ids: List[tuple],
    client,
    input_time_queue: Queue,
    retries: int,
    min_gap: float,
    buff: float,
    output_filename_queue: Queue,
    poison_queue: Queue,
    temp_stream_dir: str,
    full_stream_dir: str = None,
    pre_process: bool = False,
    parallel_process: bool = True,
    process_cores: int = None,
    daylong: bool = False,
    overlap: Union[str, float] = "calculate",
    ignore_length: bool = False,
    ignore_bad_data: bool = False,
    filt_order: int = None,
    highcut: float = None,
    lowcut: float = None,
    samp_rate: float = None,
    process_length: float = None,
):
    while True:
        killed = _check_for_poison(poison_queue)
        # Wait until output queue is empty to limit rate and memory use
        tic = default_timer()
        while output_filename_queue.full():
            # Keep on checking while we wait
            killed = _check_for_poison(poison_queue)
            if killed:
                break
            waited = default_timer() - tic
            if waited > 60:
                Logger.debug("Waiting for output_queue to not be full")
                tic = default_timer()
        if killed:
            break
        try:
            next_times = _get_and_check(input_time_queue, poison_queue)
            if next_times is None:
                break
            if isinstance(next_times, Poison):
                Logger.error("Killed")
                break
            starttime, endtime = next_times

            st = _download_st(
                starttime=starttime, endtime=endtime, buff=buff,
                min_gap=min_gap, template_channel_ids=template_channel_ids,
                client=client, retries=retries)
            if len(st) == 0:
                Logger.warning(f"No suitable data between {starttime} "
                               f"and {endtime}, skipping")
                continue
            Logger.info(f"Downloaded stream of {len(st)} traces:")
            for tr in st:
                Logger.info(tr)
            # Try to reduce memory consumption by getting rid of st if we can
            if full_stream_dir:
                for tr in st:
                    tr.split().write(os.path.join(
                        full_stream_dir,
                        f"full_trace_{tr.id}_"
                        f"{tr.stats.starttime.strftime('%y-%m-%dT%H-%M-%S')}"
                        f".ms"), format="MSEED")
            if not pre_process:
                st_chunks = [st]
            else:
                template_ids = set(['.'.join(sid)
                                    for sid in template_channel_ids])
                # Group_process copies the stream.
                st_chunks = _pre_process(
                    st=st, template_ids=template_ids, pre_processed=False,
                    filt_order=filt_order, highcut=highcut,
                    lowcut=lowcut, samp_rate=samp_rate,
                    process_length=process_length,
                    parallel=parallel_process, cores=process_cores,
                    daylong=daylong, ignore_length=ignore_length,
                    overlap=overlap, ignore_bad_data=ignore_bad_data)
                # We don't need to hold on to st!
                del st
            for chunk in st_chunks:
                Logger.info(f"After processing stream has {len(chunk)} traces:")
                for tr in chunk:
                    Logger.info(tr)
                if not os.path.isdir(temp_stream_dir):
                    os.makedirs(temp_stream_dir)
                chunk_file = os.path.join(
                    temp_stream_dir,
                    f"chunk_{len(chunk)}_"
                    f"{chunk[0].stats.starttime.strftime('%Y-%m-%dT%H-%M-%S')}"
                    f"_{os.getpid()}.pkl")
                # Add PID to cope with multiple instances operating at once
                _pickle_stream(chunk, chunk_file)
                output_filename_queue.put(chunk_file)
                del chunk
        except Exception as e:
            Logger.error(f"Caught exception {e} in downloader")
            poison_queue.put(e)
            break
    output_filename_queue.put(None)
    return


def _pre_processor(
    input_stream_queue: Queue,
    temp_stream_dir: str,
    template_ids: set,
    pre_processed: bool,
    filt_order: int,
    highcut: float,
    lowcut: float,
    samp_rate: float,
    process_length: float,
    parallel: bool,
    cores: int,
    daylong: bool,
    ignore_length: bool,
    overlap: float,
    ignore_bad_data: bool,
    output_filename_queue: Queue,
    poison_queue: Queue,
):
    while True:
        killed = _check_for_poison(poison_queue)
        if killed:
            break
        Logger.debug("Getting stream from queue")
        st = _get_and_check(input_stream_queue, poison_queue)
        if st is None:
            Logger.info("Ran out of streams, stopping processing")
            break
        elif isinstance(st, Poison):
            Logger.error("Killed")
            break
        if len(st) == 0:
            break
        Logger.info(f"Processing stream:\n{st}")

        # Process stream
        try:
            st_chunks = _pre_process(
                st, template_ids, pre_processed, filt_order, highcut, lowcut,
                samp_rate, process_length, parallel, cores, daylong,
                ignore_length, ignore_bad_data, overlap)
            for chunk in st_chunks:
                if not os.path.isdir(temp_stream_dir):
                    os.makedirs(temp_stream_dir)
                chunk_file = os.path.join(
                    temp_stream_dir,
                    f"chunk_{len(chunk)}_"
                    f"{chunk[0].stats.starttime.strftime('%Y-%m-%dT%H-%M-%S')}"
                    f"_{os.getpid()}.pkl")
                # Add PID to cope with multiple instances operating at once
                _pickle_stream(chunk, chunk_file)
                output_filename_queue.put(chunk_file)
                del chunk
        except Exception as e:
            Logger.error(
                f"Caught exception in processor:\n {e}")
            poison_queue.put(e)
            traceback.print_tb(e.__traceback__)
    output_filename_queue.put(None)
    return


def _prepper(
    input_stream_queue: Queue,
    templates: Union[List, dict],
    group_size: int,
    groups: Iterable[Iterable[str]],
    output_queue: Queue,
    poison_queue: Queue,
    xcorr_func: str = None,
):
    if isinstance(templates, dict):
        # We have been passed a db of template files on disk
        Logger.info("Deserializing templates from disk")
        try:
            templates = _read_template_db(templates)
        except Exception as e:
            Logger.error(f"Could not read from db due to {e}")
            poison_queue.put(e)
            return

    while True:
        killed = _check_for_poison(poison_queue)
        if killed:
            Logger.info("Killed in prepper")
            break
        Logger.info("Getting stream from queue")
        st_file = _get_and_check(input_stream_queue, poison_queue)
        if st_file is None:
            Logger.info("Got None for stream, prepper complete")
            break
        elif isinstance(st_file, Poison):
            Logger.error("Killed")
            break
        if isinstance(st_file, Stream):
            Logger.info("Stream provided")
            st = st_file
            # Write temporary cache of file
            st_file = tempfile.NamedTemporaryFile().name
            Logger.info(f"Writing temporary stream file to {st_file}")
            try:
                _pickle_stream(st, st_file)
            except Exception as e:
                Logger.error(
                    f"Could not write temporary file {st_file} due to {e}")
                poison_queue.put(e)
                break
        Logger.info(f"Reading stream from {st_file}")
        try:
            st = _unpickle_stream(st_file)
        except Exception as e:
            Logger.error(f"Error reading {st_file}: {e}")
            poison_queue.put(e)
            break
        st_sids = {tr.id for tr in st}
        if len(st_sids) < len(st):
            _sids = [tr.id for tr in st]
            _duplicate_sids = {
                sid for sid in st_sids if _sids.count(sid) > 1}
            poison_queue.put(NotImplementedError(
                f"Multiple channels in continuous data for "
                f"{', '.join(_duplicate_sids)}"))
            break
        # Do the grouping for this stream
        Logger.info(f"Grouping {len(templates)} templates into groups "
                    f"of {group_size} templates")
        try:
            template_groups = _group(sids=st_sids, templates=templates,
                                     group_size=group_size, groups=groups)
        except Exception as e:
            Logger.error(e)
            poison_queue.put(e)
            break
        Logger.info(f"Grouped into {len(template_groups)} groups")
        for i, template_group in enumerate(template_groups):
            killed = _check_for_poison(poison_queue)
            if killed:
                break
            try:
                template_streams = [
                    _quick_copy_stream(t.st) for t in template_group]
                template_names = [t.name for t in template_group]

                # template_names, templates = zip(*template_group)
                Logger.info(
                    f"Prepping {len(template_streams)} "
                    f"templates for correlation")
                # We can just load in a fresh copy of the stream!
                _st, template_streams, template_names = \
                    _prep_data_for_correlation(
                        stream=_unpickle_stream(st_file).merge(),
                        templates=template_streams,
                        template_names=template_names)
                if len(_st) == 0:
                    Logger.error(
                        f"No traces returned from correlation prep: {_st}")
                    continue
                starttime = _st[0].stats.starttime

                if xcorr_func in (None, "fmf", "fftw"):
                    array_dict_tuple = _get_array_dicts(
                        template_streams, _st, stack=True)
                    stream_dict, template_dict, pad_dict, \
                        seed_ids = array_dict_tuple
                    if xcorr_func == "fmf":
                        Logger.info("Prepping data for FMF")
                        # Work out used channels here
                        tr_chans = np.array(
                            [~np.isnan(template_dict[seed_id]).any(axis=1)
                             for seed_id in seed_ids])
                        no_chans = np.sum(np.array(tr_chans).astype(int),
                                          axis=0)
                        chans = [[] for _i in range(len(templates))]
                        for seed_id, tr_chan in zip(seed_ids, tr_chans):
                            for chan, state in zip(chans, tr_chan):
                                if state:
                                    chan.append((seed_id.split('.')[1],
                                                 seed_id.split('.')[-1].split(
                                                     '_')[0]))
                        # Reshape
                        t_arr, d_arr, weights, pads = _fmf_reshape(
                            template_dict=template_dict,
                            stream_dict=stream_dict,
                            pad_dict=pad_dict, seed_ids=seed_ids)
                        # Stabilise
                        t_arr, d_arr, multipliers = _fmf_stabilisation(
                            template_arr=t_arr, data_arr=d_arr)
                        # Put into queue
                        output_queue.put(
                            (starttime, i, d_arr, template_names, t_arr,
                             weights, pads, chans, no_chans))
                    else:
                        Logger.info("Prepping data for FFTW")
                        output_queue.put((
                            starttime, i, stream_dict, template_names,
                            template_dict, pad_dict, seed_ids))
                else:
                    Logger.info("Prepping data for standard correlation")
                    output_queue.put(
                        (starttime, i, _st, template_names, template_streams))
            except Exception as e:
                Logger.error(f"Caught exception in Prepper: {e}")
                traceback.print_tb(e.__traceback__)
                poison_queue.put(e)
            i += 1
        Logger.info(f"Removing temporary {st_file}")
        os.remove(st_file)
    output_queue.put(None)
    return


def _make_detections(
    input_queue: Queue,
    delta: float,
    templates: Union[List, dict],
    threshold: float,
    threshold_type: str,
    save_progress: bool,
    output_queue: Queue,
    poison_queue: Queue,
):
    chunk_id = 0
    while True:
        killed = _check_for_poison(poison_queue)
        if killed:
            break
        try:
            next_item = _get_and_check(input_queue, poison_queue)
            if next_item is None:
                Logger.info("_make_detections got None, stopping")
                break
            elif isinstance(next_item, Poison):
                Logger.error("Killed")
                break
            starttime, all_peaks, thresholds, no_chans, \
                chans, template_names = next_item
            detections = _detect(
                template_names=template_names, all_peaks=all_peaks,
                starttime=starttime, delta=delta, no_chans=no_chans,
                chans=chans, thresholds=thresholds)
            Logger.info(f"Built {len(detections)}")
            chunk_file = _make_party(
                detections=detections, threshold=threshold,
                threshold_type=threshold_type, templates=templates,
                chunk_start=starttime, chunk_id=chunk_id,
                save_progress=save_progress)
            chunk_id += 1
            output_queue.put(chunk_file)
        except Exception as e:
            Logger.error(
                f"Caught exception in detector:\n {e}")
            traceback.print_tb(e.__traceback__)
            poison_queue.put(e)
    output_queue.put(None)
    return


def _reconstruct_party(
    input_queue: Queue,
    output_queue: Queue,
    poison_queue: Queue,
    clean: bool = True,
):
    from eqcorrscan.core.match_filter.party import Party

    party = Party()
    while True:
        killed = _check_for_poison(poison_queue)
        if killed:
            break
        try:
            _chunk_file = _get_and_check(input_queue, poison_queue)
            if _chunk_file is None:
                break
            elif isinstance(_chunk_file, Poison):
                Logger.error("Killed")
                break
            Logger.info(f"Adding party from {_chunk_file} to party")
            with open(_chunk_file, "rb") as _f:
                party += pickle.load(_f)
            if clean:
                os.remove(_chunk_file)
            Logger.info(f"Added party from {_chunk_file}, party now "
                        f"contains {len(party)} detections")
        except Exception as e:
            Logger.error(f"Caught exception in party reconstructer {e}")
            traceback.print_tb(e.__traceback__)
            poison_queue.put(e)
    output_queue.put(party)
    output_queue.put(None)
    return


if __name__ == "__main__":
    import doctest

    doctest.testmod()
